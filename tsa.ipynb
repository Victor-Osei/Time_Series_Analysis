{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Title: \n",
    "##### Time Series Analysis: \"Demand Forecasting for Inventory Optimization at Corporation Favorita\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business Understanding\n",
    "#### Business Scenario\n",
    "As data scientists in Corporation Favorita, a large Ecuadorian-based grocery retailer, we are tasked to ensure that there is always the right quantity of products in stock.\n",
    "To do this we have decided to build a series of machine learning models to forecast the demand of products in various locations.We have been provided with some datasets to help in this project.\n",
    "\n",
    "\n",
    "\n",
    "#### Project Description\n",
    "This project aims to ensure optimal inventory levels at Corporation Favorita, by leveraging machine learning models to forecast product demand across various locations. Accurate demand forecasting will help maintain the right quantity of products in stock, reducing instances of overstocking and stockouts, thereby enhancing customer satisfaction and minimizing operational costs. The project follows the CRISP-DM framework and utilizes data provided by the marketing and sales teams to develop and validate predictive models\n",
    "\n",
    "\n",
    "#### Business Objective\n",
    "The primary objective of this project is to develop and implement a series of machine learning models to accurately forecast the demand for various products across different locations of Corporation Favorita. By achieving this objective, Corporation Favorita aims to optimize its inventory management, ensuring that the right quantity of products is consistently in stock. \n",
    "\n",
    "\n",
    "\n",
    "#### Hypothesis Testing\n",
    "Null Hypothesis (H0): Promotional activities and Sales do not have a significant impact on product demands in various stores. \n",
    "\n",
    "Alternate Hypothesis (H1): Sales data has a significant impact on product demands in various stores.\n",
    "\n",
    "Alternate Hypothesis (H2): Promotional activities have a significant impact on product demands in various stores. \n",
    "\n",
    "#### Analytical Questions\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "2. Which dates have the lowest and highest sales for each year (excluding days the store was closed)?\n",
    "3. Compare the sales for each month across the years and determine which month of which year had the highest sales.\n",
    "4. Did the earthquake impact sales?\n",
    "5. Are certain stores or groups of stores selling more products? (Cluster, city, state, type)\n",
    "6. Are sales affected by promotions, oil prices and holidays?\n",
    "7. What analysis can we get from the date and its extractable features?\n",
    "8. Which product family and stores did the promotions affect.\n",
    "9. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "10. Does the payment of wages in the public sector on the 15th and last days of the month influence the store sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Understanding\n",
    "#### Sourcing the Dataset\n",
    "The datasets were sourced from a github repository, a onedrive account, and a SQL server database.\n",
    "\n",
    "The data at a github repository contains two dattasets; train and transactions\n",
    "\n",
    "The data at a onedrive  was downloaded manually due to permission issues and contains two datasets also. This is to be used for testing purposes.\n",
    "\n",
    "The datasets hosted by a SQL server database was queried, and the respective dataframes saved as single files in csv format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install required packages\n",
    "# !pip install statsmodels\n",
    "\n",
    "\n",
    "#Libraries for sql\n",
    "# database connections\n",
    "import pyodbc    \n",
    "from dotenv import dotenv_values\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "#libraries for handling data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##data visualizations\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "import calplot\n",
    "\n",
    "# Feature Processing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#stat models\n",
    "# from pmdarima import auto_arima\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from statsmodels.graphics.tsaplots import plot_pacf \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Error evaluations\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error,mean_squared_log_error, mean_absolute_error\n",
    "\n",
    "# Modelling\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from SQL Server database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading first dataset from database\n",
    "# Load environment variables from .env file\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Access database credentials from environment variables dictionary\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f\"DRIVER=ODBC Driver 17 for SQL Server;SERVER={server};DATABASE={database};User Id={username};PASSWORD={password};\"\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f\"DRIVER=ODBC Driver 17 for SQL Server;SERVER={server};DATABASE={database};UID={username};PWD={password};\"\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = pyodbc.connect(connection_string)\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# Specify the SQL queries to extract data from the tables\n",
    "oil_data = \"SELECT * FROM dbo.oil\"\n",
    "holiday_data = \"SELECT * FROM dbo.holidays_events\"\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a cursor from the connection\n",
    "with connection.cursor() as cursor:\n",
    "    # Execute the queries and fetch data into Pandas DataFrames\n",
    "    oil_data = pd.read_sql_query(oil_data, connection)\n",
    "    holiday_data = pd.read_sql_query(holiday_data, connection)\n",
    "    store_data = pd.read_sql_query(store_data, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_data = pd.read_csv(\"holiday_data.csv\")\n",
    "holiday_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_data = pd.read_csv(\"stores_data.csv\")\n",
    "stores_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_data = pd.read_csv(\"oil_data.csv\")\n",
    "oil_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
